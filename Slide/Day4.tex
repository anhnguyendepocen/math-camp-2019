\documentclass[pdflatex, 12pt]{beamer}
\usetheme{Boadilla}
\usefonttheme{professionalfonts}

\usepackage{graphicx}
\usepackage{color}
\usepackage{amsmath, amssymb}
\usepackage{bm}
%\usepackage{enumitem}
\usepackage{natbib}
\usepackage{url}
\usepackage{wasysym}
\usepackage{setspace}

%\setbeamerfont{title}{series=\bfseries}
%\setbeamerfont{frametitle}{series=\bfseries}

%\setbeamercolor{title}{fg=violet}
%\setbeamercolor{frametitle}{fg=violet}

%\setbeamercolor{palette primary}{fg=black, bg=violet!50}
%\setbeamercolor{palette secondary}{fg=violet, bg=white}
%\setbeamercolor{palette tertiary}{fg=black, bg=violet!70}

\setbeamertemplate{navigation symbols}{}

%\setbeamertemplate{itemize item}{\color{violet}$\bullet$}
%\setbeamertemplate{itemize subitem}{\color{violet}\scriptsize{$\blacktriangleright$}}
%\setbeamertemplate{itemize subsubitem}{\color{violet}$\star$}

\setbeamertemplate{itemize item}{$\bullet$}
\setbeamertemplate{itemize subitem}{\scriptsize{$\blacktriangleright$}}
\setbeamertemplate{itemize subsubitem}{$\star$}

\setbeamertemplate{enumerate items}[default]

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}

\title[Math Camp: Day 4]{Day 4: Vectors and Matrices}
\author[Ikuma Ogura]{Ikuma Ogura}
\institute[Georgetown]{Ph.D. student, Department of Government, Georgetown University}
\date[August 22, 2019]{August 22, 2019}

\begin{document}

\begin{frame}
\frametitle{}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Today}
\begin{itemize}
\item Today
 \begin{itemize}
 \item Vector \& matrix algebra
 \item Matrix calculus
 \item Geometry of matrix algebra
 \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Why Vectors and Matrices?}
\begin{itemize}
\item Making notations \& calculations simple
\vspace{0.4cm}
\item Basis of multivariate statistical techniques
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Why Vectors and Matrices? (cont.)}
\begin{itemize}
\item We have so far dealt with algebra and calculus with \textbf{scalars}
\vspace{0.4cm}
\item Suppose we want to use more than one independent variable (say 10) in regression analysis:
 \begin{equation}
 y_i = b_0 + b_1 x_{1i} + b_2 x_{2i} + b_3 x_{3i} + b_4 x_{4i} + b_5 x_{5i} + \cdots + b_{10} x_{10i} + e_i \notag
 \end{equation}
\item Let's find $b_0, b_1, b_2, \cdots, b_{10}$ which minimize the sum of squared residuals
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Why Vectors and Matrices? (cont.)}
\begin{itemize}
\item As we covered yesterday, we need to solve the system of equations
 \begin{eqnarray}
 \begin{cases}
 \frac{\partial}{\partial b_{0}} \sum_{i = 1}^n \left\{y_i - (b_0 + b_1 x_{1i} + b_2 x_{2i} + \cdots + b_{10} x_{10i})\right\}^2 = 0 \\
 \frac{\partial}{\partial b_{1}} \sum_{i = 1}^n \left\{y_i - (b_0 + b_1 x_{1i} + b_2 x_{2i} + \cdots + b_{10} x_{10i})\right\}^2 = 0 \\
 \ \ \ \vdots \\
 \frac{\partial}{\partial b_{10}} \sum_{i = 1}^n \left\{y_i - (b_0 + b_1 x_{1i} + b_2 x_{2i} + \cdots + b_{10} x_{10i})\right\}^2 = 0 \notag
 \end{cases}
 \end{eqnarray}
\vspace{0.2cm}
\item Aww...!
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Tips}
\begin{itemize}
\item Keep track of vector/matrix dimensions.
\vspace{0.4cm}
\item Become able to connect with scalar algebra/calculus
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Vector}
\begin{itemize}
\item A $k$-dimensional vector is a list of $k$ numbers.
\vspace{0.4cm}
\item Usually numbers are arranged in a column.
\vspace{0.4cm}
\item We usually represent a vector using a bold lower case. e.g., $\bm{a}$ 
 \begin{equation}
 \bm{a} = \begin{pmatrix}
 a_1 \\
 a_2 \\
 \vdots \\
 a_k
 \end{pmatrix} \notag
 \end{equation}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Vector (cont.)}
\begin{itemize}
\item To arrange numbers in a row, we {\color{red} transpose} the vector.
\vspace{0.4cm}
\item Transpose of a column vector $\bm{a}$ of dimension $k$, denoted as $\bm{a}'$ (also written as $\bm{a}^T$), is a row vector
 \begin{equation}
 \bm{a}' = \begin{pmatrix}
 a_1 & a_2 \ldots & a_k \\
 \end{pmatrix} \notag
 \end{equation}
\item Obviously, the transpose of a row vector is a column vector!
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Vector (cont.)}
\begin{itemize}
\item {\color{red} Norm} (or length) of vector $\bm{a}$ is defined as 
 \begin{equation}
 \|\bm{a}\| = \sqrt{a_1^2 + a_2^2 + \cdots + a_k^2} \notag
 \end{equation}
\item Related types of vectors
 \begin{itemize}
 \item Normalized vector: a vector with norm 1
 \item Zero vector ($\bm{0}$): a vector whose elements are all 0
 \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix}
\begin{itemize}
\item A $n \times k$-dimensional matrix is a rectangle array of numbers
 \begin{equation}
 \bm{A} = \begin{pmatrix}
 a_{11} & a_{12} & \ldots & a_{1k} \\
 a_{21} & a_{22} & \ldots & a_{2k} \\
 \vdots & \vdots & \ddots & \vdots \\
 a_{n1} & a_{n2} & \ldots & a_{nk} \\
 \end{pmatrix} \notag
 \end{equation}
\item We usually represent a matrix using a bold upper case. e.g., $\bm{A}$
\vspace{0.4cm}
\item By convention, $a_{ij}$ refers to an element in the $i$th row and the $j$th column.
\vspace{0.4cm}
\item We can think of a $k$-dimensional column vector as a $k \times 1$ matrix and a $k$-dimensional row vector as a $1 \times k$ matrix.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix (cont.)}
\begin{itemize}
\item It is often useful to think of matrices as made up of a collection of column/row vectors.
\vspace{0.4cm}
\item For example, we can represent matrix $\bm{A}$ as a collection of column vectors.
 \begin{equation}
 \bm{A} = \begin{pmatrix}
 a_{11} & a_{12} & \ldots & a_{1k} \\
 a_{21} & a_{22} & \ldots & a_{2k} \\
 \vdots & \vdots & \ddots & \vdots \\
 a_{n1} & a_{n2} & \ldots & a_{nk} \\
 \end{pmatrix} = \begin{pmatrix}
 \bm{a_1} & \bm{a_2} & \ldots & \bm{a_k} \\
 \end{pmatrix} \notag
 \end{equation}
 where
 \begin{equation}
 \bm{a_i} = \begin{pmatrix}
 a_{1i} \\
 a_{2i} \\
 \vdots \\
 a_{ni} \\
 \end{pmatrix} \notag
 \end{equation}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix (cont.)}
\begin{itemize}
\item Similarly, we can represent matrix $\bm{A}$ as a collection of row vectors.
 \begin{equation}
 \bm{A} = \begin{pmatrix}
 a_{11} & a_{12} & \ldots & a_{1k} \\
 a_{21} & a_{22} & \ldots & a_{2k} \\
 \vdots & \vdots & \ddots & \vdots \\
 a_{n1} & a_{n2} & \ldots & a_{nk} \\
 \end{pmatrix} = \begin{pmatrix}
 \bm{\alpha_1}' \\
 \bm{\alpha_2}' \\
 \vdots \\
 \bm{\alpha_n}' \\
 \end{pmatrix} \notag
 \end{equation}
where $\bm{\alpha_i}$ is a vector where elements of $i$ th row of $\bm{A}$ are arranged in a column,
 \begin{equation}
 \bm{\alpha_i} = \begin{pmatrix}
 a_{i1} & a_{i2} & \ldots & a_{ik} \\
 \end{pmatrix}'  = \begin{pmatrix}
 a_{i1} \\
 a_{i2} \\
 \vdots \\
 a_{ik}
 \end{pmatrix} \notag
 \end{equation}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix (cont.)}
\begin{itemize}
\item Example: Matrix
 \begin{equation}
 \bm{A} = \begin{pmatrix}
 1 & 2 \\
 1 & -4 \\
 \end{pmatrix} \notag
 \end{equation}
can be repsented as a collection of column vectors
 \begin{equation}
 \bm{a_1} = \begin{pmatrix}
 1 \\
 1 \\
 \end{pmatrix}, \bm{a_2} = \begin{pmatrix}
 2 \\
 -4 \\
 \end{pmatrix} \notag
 \end{equation}
or as a collection of row vectors
 \begin{equation}
 \bm{\alpha_1} = \begin{pmatrix}
 1 & 2 \\
 \end{pmatrix}' = \begin{pmatrix}
 1 \\
 2 \\
 \end{pmatrix}, \bm{\alpha_2} = \begin{pmatrix}
 1 & -4 \\
 \end{pmatrix}' = \begin{pmatrix}
 1 \\
 -4 \\
 \end{pmatrix} \notag
 \end{equation}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix (cont.)}
\begin{itemize}
\item A transpose of a matrix can be obtained by flipping its rows and columns.
\vspace{0.4cm}
\item Transpose of a $n \times k$ matrix $\bm{A}$ is denoted as $\bm{A}'$ (also written as $\bm{A}^T$)
 \begin{equation}
 \bm{A'} = \begin{pmatrix}
 a_{11} & a_{21} & \ldots & a_{n1} \\
 a_{12} & a_{22} & \ldots & a_{n2} \\
 \vdots & \vdots & \ddots & \vdots \\
 a_{1k} & a_{2k} & \ldots & a_{nk} \\ 
 \end{pmatrix} \notag
 \end{equation} 
where the dimension of $\bm{A}'$ is $k \times n$.
\vspace{0.4cm}
\item Obviously, $(\bm{A}')' = \bm{A}$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix (cont.)}
\begin{itemize}
\item Example: The transpose of matrix 
 \begin{equation}
 \bm{A} = \begin{pmatrix}
 1 & 2 & 3 \\
 1 & -4 & -2 \\
 \end{pmatrix} \notag
 \end{equation}
is
 \begin{equation}
 \bm{A}' = \begin{pmatrix}
 1 & 1 \\
 2 & -4 \\
 3 & -2 \\
 \end{pmatrix} \notag
 \end{equation}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix Addition/Subtraction}
\begin{itemize}
\item Addition/subtraction of vectors of the same dimension is defined as
 \begin{equation}
 \bm{a} \pm \bm{b} = \begin{pmatrix}
 a_1 \pm b_1 \\
 a_2 \pm b_2 \\
 \vdots \\
 a_n \pm b_n \\
 \end{pmatrix} \notag
 \end{equation}
\item Similarly, if matrices $\bm{A}$ and $\bm{B}$ are of the same dimensions, we can define addition/subtraction as 
 \begin{equation}
 \bm{A} \pm \bm{B} = \begin{pmatrix}
 a_{11} \pm b_{11} & a_{12} \pm b_{12} & \ldots & a_{1k} \pm b_{1k} \\
 a_{21} \pm b_{21} & a_{22} \pm b_{22} & \ldots & a_{2k} \pm b_{2k} \\
 \vdots & \vdots & \ddots & \vdots \\
 a_{n1} \pm b_{n1} & a_{n2} \pm b_{n2} & \ldots & a_{nk} \pm b_{nk} \\
 \end{pmatrix} \notag
 \end{equation}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix Multiplication}
\begin{itemize}
\item We can multiply a vector $\bm{a}$ by a scalar $c$ as 
 \begin{equation}
 c\bm{a} = \begin{pmatrix}
 ca_1 \\
 ca_2 \\
 \vdots \\
 ca_k
 \end{pmatrix} \notag
 \end{equation}
\item Similarly, we can define the scalar multiplication of a matrix $\bm{A}$ by a scalar $c$ as
 \begin{equation}
 c\bm{A} = \begin{pmatrix}
 ca_{11} & ca_{12} & \ldots & ca_{1k} \\
 ca_{21} & ca_{22} & \ldots & ca_{2k} \\
 \vdots & \vdots & \ddots & \vdots \\
 ca_{n1} & ca_{n2} & \ldots & ca_{nk} \\
 \end{pmatrix} \notag
 \end{equation}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix Multiplication (cont.)}
\begin{itemize}
\item {\color{red} Dot product}/{\color{red} inner product} for vectors of the same dimensions, $\bm{a}$ and $\bm{b}$, is defined as 
\begin{equation}
 \bm{a} \cdot \bm{b} = \bm{b} \cdot \bm{a} = a_{1}b_{1} + a_{2}b_{2} + \cdots + a_{k}b_{k} = \sum_j^k a_{j}b_{j} \notag 
 \end{equation}
Therefore, an inner product of two vectors is a scalar.
\vspace{0.4cm}
\item Vector norm can be also expressed as $\|\bm{a}\| = \sqrt{\bm{a} \cdot \bm{a}}$.  
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix Multiplication (cont.)}
\begin{itemize}
\item If $\bm{A}$ is a $n \times k$ matrix and $\bm{B}$ is a $k \times m$ matrix, then we can define their product $\bm{C} = \bm{AB}$ where
 \begin{equation}
 c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \cdots + a_{ik}b_{kj} \notag
 \end{equation}
 \begin{itemize}
 \item $ij$ element of the resultant matrix $\bm{C}$ is the innner product of $i$th row of $\bm{A}$ and $j$th column of $\bm{B}$.
 \item Therefore, $\bm{C}$ is a $n \times m$ matrix.
 \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix Multiplication (cont.)}
\begin{itemize}
\item Example: Let
 \begin{equation}
 \bm{A} = \begin{pmatrix}
 3 & 8 \\
 -5 & 2 \\
 \end{pmatrix}, \bm{B} = \begin{pmatrix}
 -4 & 6 \\
 2 & -7
 \end{pmatrix}. \notag
 \end{equation}
Then their product $\bm{AB}$ can be calculated as 
 \only<1>{
 \begin{equation}
 \bm{AB} = \begin{pmatrix}
 {\color{red} 3} & {\color{red}8} \\
 -5 & 2 \\
 \end{pmatrix} \begin{pmatrix}
 {\color{red}-4} & 6 \\
 {\color{red}2} & -7 \\
 \end{pmatrix} = \begin{pmatrix}
 {\color{red}3 \cdot (-4) + 8 \cdot 2} & \ \\
  & \ \\
 \end{pmatrix} \notag
 \end{equation}
 }
 \only<2>{
 \begin{equation}
 \bm{AB} = \begin{pmatrix}
 {\color{red}3} & {\color{red}8} \\
 -5 & 2 \\
 \end{pmatrix} \begin{pmatrix}
 -4 & {\color{red}6} \\
 2 & {\color{red}-7} \\
 \end{pmatrix} = \begin{pmatrix}
 2 & {\color{red}3 \cdot 6 + 8 \cdot (-7)} \\
  & \ \\
 \end{pmatrix} \notag
 \end{equation}
 }
 \only<3>{
 \begin{equation}
 \bm{AB} = \begin{pmatrix}
 3 & 8 \\
 {\color{red}-5} & {\color{red}2} \\
 \end{pmatrix} \begin{pmatrix}
 {\color{red}-4} & 6 \\
 {\color{red}2} & -7 \\
 \end{pmatrix} = \begin{pmatrix}
 2 & -38 \\
 {\color{red}(-5) \cdot (-4) + 2 \cdot 2}  & \ \\
 \end{pmatrix} \notag
 \end{equation}
 }
 \only<4>{
 \begin{equation}
 \bm{AB} = \begin{pmatrix}
 3 & 8 \\
 {\color{red}-5} & {\color{red}2} \\
 \end{pmatrix} \begin{pmatrix}
 -4 & {\color{red}6} \\
 2 & {\color{red}-7} \\
 \end{pmatrix} = \begin{pmatrix}
 2 & -38 \\
 24 & {\color{red}(-5) \cdot 6 + 2 \cdot (-7)} \\
 \end{pmatrix} \notag
 \end{equation}
 }
 \only<5>{
 \begin{equation}
 \bm{AB} = \begin{pmatrix}
 3 & 8 \\
 -5 & 2 \\
 \end{pmatrix} \begin{pmatrix}
 -4 & 6 \\
 2 & -7 \\
 \end{pmatrix} = \begin{pmatrix}
 2 & -38 \\
 24 & -44 \\
 \end{pmatrix} \notag
 \end{equation}
 }
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix Multiplication (cont.)}
\begin{itemize}
\item $\bm{AB}$ is generally not equal to $\bm{BA}$, even if both are defined.
\vspace{0.4cm}
\item Common properties of transpose matrices
 \begin{enumerate}
 \item $(\bm{AB})' = \bm{B}'\bm{A}'$
 \item $(\bm{A}'\bm{A})' = \bm{A}'(\bm{A}')' = \bm{A'A}$
 \end{enumerate}
\vspace{0.4cm}
\item Thinking of vectors $\bm{a}$ and $\bm{b}$ as $k \times 1$ matrices, their inner product can also be written as their product
 \begin{equation}
 \bm{a} \cdot \bm{b} = \bm{a}'\bm{b} = \begin{pmatrix}
 a_1 & a_2 & \ldots & a_k \\
 \end{pmatrix} \begin{pmatrix}
 b_1 \\
 b_2 \\
 \vdots \\
 b_k
 \end{pmatrix} \notag 
 \end{equation}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix Calculation: Exercises}
\begin{itemize}
\item Let 
 \begin{equation}
 \bm{A} = \begin{pmatrix}
 2 & 5 \\
 -3 & 1 \\
 \end{pmatrix}, \bm{B} = \begin{pmatrix}
 4 & -2 & 7 \\
 -3 & 1 & 0 \\
 \end{pmatrix}, \notag
 \end{equation}
 \begin{equation}
 \bm{C} = \begin{pmatrix}
 5 & -1 \\
 0 & 4 \\
 -2 & 1 \\
 \end{pmatrix}, \bm{D} = \begin{pmatrix}
 2 & 3 \\
 -1 & 2 \\
 \end{pmatrix}. \notag
 \end{equation}
Calculate the following.
 \begin{enumerate}
 \item $\bm{B}' + \bm{C}$
 \item $\bm{AD}$
 \item $\bm{DA}$
 \item $\bm{BC}$
 \item $\bm{B}'\bm{C}'$
 \end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix Types}
\begin{itemize}
\item {\color{red}{Square Matrix}}: Number of rows and columns are the same. 
\vspace{0.4cm}
\item {\color{red}{Symmetric Matrix}}: A square matrix where $\bm{A} = \bm{A'}$. Therefore, $a_{ij} = a_{ji}$.
\vspace{0.4cm}
\item {\color{red}{Triangular Matrix}}: A square matrix in which all the elements above or below the main diagonal are equal to 0.
 \begin{itemize}
 \item Main diagonal: $a_{ij}$s where $i = j$
 \item A square matrix where elements above the main diagonal are 0 are called {\color{red}lower triangular}, and one where elements below the main diagonal equal 0 are called {\color{red} upper triangular}.
 \end{itemize}
\vspace{0.4cm}
\item {\color{red}{Diagonal Matrix}}: A square matrix where off-diagonal elements are all 0.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix Types (cont.)}
\begin{itemize}
\item {\color{red}{Identity Matrix}}: A diagonal matrix where all diagonal elements are 1.
 \begin{equation}
 \bm{I} = \begin{pmatrix}
 1 & 0 & \ldots & 0 \\
 0 & 1 & \ldots & 0 \\
 \vdots & \vdots & \ddots & \vdots \\
 0 & 0 & \ldots & 1 \\
 \end{pmatrix} \notag
 \end{equation}
\item The identity matrix works like scalar 1. That is, for any matrices $\bm{A}$ that are conformable with $\bm{I}$, 
 \begin{equation}
 \bm{AI} = \bm{IA} = \bm{A} \notag
 \end{equation}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix Types: Exercises}
\begin{itemize}
\item Are the following matrices square, symmetric, triangular, and diagonal?  
 \begin{enumerate}
 \item $\begin{pmatrix}
 1 & 1 \\
 3 & -2 \\
 \end{pmatrix}$
 \vspace{0.2cm}
 \item $\begin{pmatrix}
 3 & 6 & 0 \\
 6 & 5 & -7 \\
 0 & -7 & 0 \\
 \end{pmatrix}$
 \vspace{0.2cm}
 \item $\begin{pmatrix}
 1 & 4 & 1 \\
 0 & 6 & -5 \\
 0 & 0 & 1 \\
 \end{pmatrix}$
 \end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Trace}
\begin{itemize}
\item The trace of a square matrix is the sum of its diagonal elements
 \begin{equation}
 \mathrm{tr}(\bm{A}) = \sum_i^k a_{ii} \notag
 \end{equation}
\item Properties of trace
 \begin{enumerate}
 \item $\mathrm{tr}(c\bm{A}) = c\mathrm{tr}(\bm{A})$
 \item $\mathrm{tr}(\bm{A}') = \mathrm{tr}(\bm{A})$
 \item $\mathrm{tr}(\bm{A} + \bm{B}) = \mathrm{tr}(\bm{A}) + \mathrm{tr}(\bm{B})$
 \item $\mathrm{tr}(\bm{AB}) = \mathrm{tr}(\bm{BA})$
 \end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Determinant}
\begin{itemize}
\item The determinant is another widely used operation	 to transform a square matrix to a scalar.
\vspace{0.4cm} 
\item The determinant of matrix $\bm{A}$ is represented as $\mathrm{det}\bm{A}$ or $|\bm{A}|$
\vspace{0.4cm}
\item Properties of determinant: let $\bm{A}$ be a $n \times n$ square matrix,
 \begin{enumerate}
 \item $\mathrm{det} \bm{A} = \mathrm{det} \bm{A}'$
 \item if $\bm{A}$ is either diagonal or triangular, $\mathrm{det} \bm{A} = \prod_{i = 1}^n a_{ii}$
 \item $\mathrm{det} (c\bm{A}) = c^n\mathrm{det} \bm{A}$
 \item $\mathrm{det} \bm{AB} = \mathrm{det} \bm{BA} = \mathrm{det} \bm{A}\mathrm{det} \bm{B}$
 \end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Inverse}
\begin{itemize}
\item If a square matrix $\bm{A}$ is {\color{red}{nonsingular}} (or {\color{red}invertible}), a square matrix $\bm{A}^{-1}$ exists which satisfies
 \begin{equation}
 \bm{AA}^{-1} = \bm{A}^{-1}\bm{A} = \bm{I} \notag
 \end{equation}
\item We call $\bm{A}^{-1}$ as the {\color{red}{inverse}} of $\bm{A}$.
\vspace{0.4cm}
\item When we cannot define $\bm{A}^{-1}$, $\bm{A}$ is called singular.
\vspace{0.4cm}
\item Some properties of the inverse
 \begin{enumerate}
 \item if $\bm{A}$ is nonsingular, $\bm{A}^{-1}$ is unique
 \item $(\bm{A}^{-1})^{-1} = \bm{A}$
 \item $(\bm{A}^{-1})' = (\bm{A}')^{-1}$
 \item $(\bm{AB})^{-1} = \bm{B}^{-1}\bm{A}^{-1}$ 
 %\item $(\bm{A + B})^{-1} = \bm{A}^{-1}(\bm{A}^{-1} + \bm{B}^{-1})^{-1}\bm{B}^{-1}$
 \item $\mathrm{det} \bm{A} \neq 0 \Leftrightarrow \bm{A}$ is nonsingular
 \end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{System of Linear Equations}
\begin{itemize}
\item Matrices help us express and solve system of linear equations.
\vspace{0.4cm}
\item For example, the system of linear equations
 \begin{eqnarray}
 \begin{cases}
 x - y = 4 \\
 2x + y = 2 \notag
 \end{cases}
 \end{eqnarray}
can be expressed as 
 \begin{equation}
 \bm{Ax} = \bm{b} \notag 
 \end{equation}
where $\bm{A} = \begin{pmatrix}
1 & -1 \\
2 & 1 \\
\end{pmatrix}$, $\bm{x} = \begin{pmatrix}
x \\
y \\
\end{pmatrix}$, and $\bm{b} = \begin{pmatrix}
4 \\
2 \\
\end{pmatrix}$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{System of Linear Equations (cont.)}
\begin{itemize}
\item More generally, a system of $n$ linear equations with $n$ unknowns 
 {\small
 \begin{eqnarray}
 \begin{cases}
 a_{11} x_1  + a_{12} x_2 + \cdots + a_{1n} x_n = b_1 \\
 a_{21} x_1  + a_{22} x_2 + \cdots + a_{2n} x_n = b_2 \\
 \ \ \ \vdots \\
 a_{n1} x_1  + a_{n2} x_2 + \cdots + a_{nn} x_n = b_n \notag
 \end{cases}
 \end{eqnarray}
 }
can be represented as 
 \begin{equation}
 \bm{Ax} = \bm{b} \notag 
 \end{equation}
where {\small $\bm{A} = \begin{pmatrix}
a_{11} & a_{12} & \ldots & a_{1n} \\
a_{21} & a_{22} & \ldots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \ldots & a_{nn} \\
\end{pmatrix}$}, {\small $\bm{x} = \begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{pmatrix}$}, and {\small $\bm{b} = \begin{pmatrix}
b_1 \\
b_2 \\
\vdots \\
b_n
\end{pmatrix}$}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{System of Linear Equations (cont.)}
\begin{itemize}
\item Example: the system of regression equations
 {\footnotesize
 \begin{eqnarray}
 \begin{cases}
 y_1 = b_0 + b_1 x_{11} + \cdots + b_k x_{1k} + e_1 & \\
 y_2 = b_0 + b_1 x_{21} + \cdots + b_k x_{2k} + e_2 & \\
 \ \ \ \vdots & \\
 y_n = b_0 + b_1 x_{n1} + \cdots + b_k x_{nk} + e_n & \notag
 \end{cases}
 \end{eqnarray}
 }
can be written as 
 \begin{equation}
 \bm{y} = \bm{Xb} + \bm{e} \notag
 \end{equation}
where {\scriptsize $\bm{y} = \begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{pmatrix}$}, {\scriptsize $\bm{X} =
\begin{pmatrix}
 1 & x_{11} & \ldots & x_{1k} \\
 1 & x_{21} &  \ldots & x_{2k} \\
 \vdots & \vdots & \ddots & \vdots \\
 1 & x_{n1} &  \ldots & x_{nk} \\
\end{pmatrix}$}, {\scriptsize $\bm{b} = \begin{pmatrix}
b_0 \\
b_1 \\
\vdots \\
b_k \\
\end{pmatrix}$}, and  {\scriptsize $\bm{e} = \begin{pmatrix}
e_1 \\
e_2 \\
\vdots \\
e_n \\
\end{pmatrix}$}.
 \vspace{0.2cm}
 \begin{itemize}
 \item Why is the elements in the first column of $\bm{X}$ are all 1? 
 \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{System of Linear Equations (cont.)}
\begin{itemize}
\item Let's solve a system of linear equations written as 
 \begin{equation}
 \bm{Ax} = \bm{b} \notag 
 \end{equation}
for $\bm{x}$.
\vspace{0.4cm}
\item Assuming that $\bm{A}$ is invertible, multiply $\bm{A}^{-1}$ from the left:
 \begin{eqnarray}
 && \bm{A}^{-1}\bm{Ax} = \bm{A}^{-1}\bm{b} \notag \\
 &\Rightarrow& \bm{x} = \bm{A}^{-1}\bm{b} \notag
 \end{eqnarray}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{System of Linear Equations (cont.)}
\begin{itemize}
\item Example: Solving the system of linear equations
 \begin{eqnarray}
 \begin{cases}
 x - y = 4 \\
 2x + y = 2 \notag
 \end{cases}
 \end{eqnarray}
using matrix inversion,
 \begin{equation}
 \begin{pmatrix}
 x \\
 y \\
 \end{pmatrix} = \begin{pmatrix}
 1 & -1 \\
 2 & 1 \\
 \end{pmatrix}^{-1} \begin{pmatrix}
 4 \\
 2 \\
 \end{pmatrix} = \begin{pmatrix}
 \frac{1}{3} & \frac{1}{3} \\
 -\frac{2}{3} & \frac{1}{3} \\
 \end{pmatrix} \begin{pmatrix}
 4 \\
 2 \\
 \end{pmatrix} = \begin{pmatrix}
 2 \\
 -2 \\
 \end{pmatrix} \notag
 \end{equation}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix Calculus: Preparation}
\begin{itemize}
\item We can also represent function $y = f(x_1, x_2, \cdots, x_n)$ using vector notation:
 \begin{equation}
 y = f(\bm{x}) \notag
 \end{equation}
where $\bm{x}$ is the vector of inputs
 \begin{equation}
 \bm{x} = \begin{pmatrix}
 x_1 \\
 x_2 \\
 \vdots \\
 x_n \\
 \end{pmatrix} \notag 
 \end{equation}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix Calculus: Preparation (cont.)}
\begin{itemize}
\item Example: linear function of $x_1, x_2, \cdots, x_n$ can be written using vectors
 \begin{equation}
 a_1 x_1 + a_2 x_2 + \cdots + a_n x_n = \begin{pmatrix}
 a_1 & a_2 & \ldots & a_n
 \end{pmatrix} \begin{pmatrix}
 x_1 \\
 x_2 \\
 \vdots \\
 x_n \\
 \end{pmatrix} = \bm{a}'\bm{x} \notag
 \end{equation}
where $\bm{a}$ is the vector of coefficients and $\bm{x}$ is the vector of variables.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix Calculus: Preparation (cont.)}
\begin{itemize}
\item Another example: {\color{red} quadratic form} is a polynomial in which each term is the monomial of degree 2, and can be written using vectors and a matrix
 \begin{equation}
 \bm{x}'\bm{Ax} = \sum_{i = j} a_{ii} x_i^2 + \sum_{i \neq j} (a_{ij} + a_{ji}) x_i x_j \notag 
 \end{equation}
where {\small $\bm{A} = \begin{pmatrix}
a_{11} & a_{12} & \ldots & a_{1n} \\
a_{21} & a_{22} & \ldots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \ldots & a_{nn} \\
\end{pmatrix}$} is the matrix of coefficients and {\small $\bm{x} = \begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{pmatrix}$} is the vector of variables.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix Calculus: Preparation (cont.)}
\begin{itemize}
\item Quadratic form of two variables $\bm{x} = \begin{pmatrix}
x_1 \\
x_2 \\
\end{pmatrix}$ and coefficient matrix $\bm{A} = \begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
\end{pmatrix}$ is
 \begin{eqnarray}
 && \begin{pmatrix}
 x_1 & x_2 \\
 \end{pmatrix} \begin{pmatrix}
 a_{11} & a_{12} \\
 a_{21} & a_{22} \\
 \end{pmatrix} \begin{pmatrix}
 x_1 \\
 x_2 \\
 \end{pmatrix} \notag \\
 &=& \begin{pmatrix}
 (a_{11} x_1 + a_{12} x_2) & (a_{21} x_1 + a_{22} x_2)
 \end{pmatrix} \begin{pmatrix}
 x_1 \\
 x_2 \\
 \end{pmatrix} \notag \\
 &=& (a_{11} x_1 + a_{12} x_2)x_1 + (a_{21} x_1 + a_{22} x_2) x_2 \notag \\
 &=& a_{11} x_1^2 + a_{22} x_2^2 + (a_{12} + a_{21}) x_1 x_2 \notag
 \end{eqnarray}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Gradient}
\begin{itemize}
\item {\color{red} Gradient} of a function $f(\bm{x})$ is a column vector of dimention $n$ whose $i$th element is the partial derivative of $f(\bm{x})$ with respect to $x_i$.
 \begin{equation}
 \nabla f(\bm{x}) = \frac{\partial}{\partial \bm{x}} f(\bm{x}) = \begin{pmatrix}
 \frac{\partial}{\partial x_1} f(\bm{x}) \\
 \frac{\partial}{\partial x_2} f(\bm{x}) \\
 \ \vdots \\
 \frac{\partial}{\partial x_n} f(\bm{x}) \\
 \end{pmatrix} \notag
 \end{equation}
 \begin{itemize}
 \item The operator $\nabla$ is called {\color{red} nabla}.
 \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Gradient (cont.)}
\begin{itemize}
\item Example: let $f(x, y) = x^2 - xy + y^2$. Then, its gradient vector is 
 \begin{equation}
 \nabla f(x, y) = \begin{pmatrix}
 2x - y \\
 -x + 2y \\
 \end{pmatrix} \notag
 \end{equation}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Gradient (cont.)}
\begin{itemize}
\item Another example: we can write the normal equation for deriving the OLS coefficients using gradient. Let $S(\bm{b})$ be the function to compute the sum of squared residuals, where $\bm{b}$ is the vector of coefficients. Then, 
 \begin{equation}
 \nabla S(\bm{b}) = \frac{\partial}{\partial \bm{b}} S(\bm{b}) = \begin{pmatrix}
 \frac{\partial}{\partial b_0} S(\bm{b}) \\
 \frac{\partial}{\partial b_1} S(\bm{b}) \\
 \ \vdots \\
 \frac{\partial}{\partial b_k} S(\bm{b}) \\
 \end{pmatrix} = \bm{0} \notag
 \end{equation}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Hessian}
\begin{itemize}
\item {\color{red} Hessian} of function $f(\bm{x})$ is a matrix whose $ij$ entry is the second-order derivative of $f(\bm{x})$ with regard to $x_i$ and $x_j$.
 {\small
 \begin{equation}
 \bm{H} = \frac{\partial^2}{\partial \bm{x} \partial \bm{x}'} f(\bm{x}) = \begin{pmatrix}
 \frac{\partial^2}{\partial x_1^2} f(\bm{x}) & \frac{\partial^2}{\partial x_1 \partial x_2} f(\bm{x}) & \ldots & \frac{\partial^2}{\partial x_1 \partial x_k} f(\bm{x}) \\
 \frac{\partial^2}{\partial x_2 \partial x_1} f(\bm{x}) & \frac{\partial^2}{\partial x_2^2} f(\bm{x}) & \ldots & \frac{\partial^2}{\partial x_2 \partial x_k} f(\bm{x}) \\
 \vdots & \vdots & \ddots & \vdots \\
 \frac{\partial^2}{\partial x_k \partial x_1} f(\bm{x}) & \frac{\partial^2}{\partial x_k \partial x_2} f(\bm{x}) & \ldots & \frac{\partial^2}{\partial x_k^2} f(\bm{x}) \\
 \end{pmatrix} \notag
 \end{equation}
 }
\item Based on the discussion yesterday... $\rightarrow$ Hessian is a symmetric matrix
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Rules for Matrix Calculus}
\begin{itemize}
\item Rules for linear functions and qudratic forms: let $\bm{a}$ and $\bm{A}$ be vector/matrix of coefficients and $\bm{x}$ the vector of variables, then
 \begin{enumerate}
 \item $\frac{\partial}{\partial \bm{x}} (\bm{a'x}) = \frac{\partial}{\partial \bm{x}} (\bm{x'a}) = \bm{a}$
 \item $\frac{\partial}{\partial \bm{x}} (\bm{x'Ax}) = (\bm{A} + \bm{A'})\bm{x}$
 \item $\frac{\partial^2}{\partial \bm{x}\partial \bm{x}'} (\bm{x'Ax}) = \bm{A} + \bm{A'}$
 \end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Rules for Matrix Calculus: Exercise}
\begin{itemize}
\item Let $\bm{x} = \begin{pmatrix}
x_1 \\
x_2 \\
\end{pmatrix}$ and $\bm{A} = \begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
\end{pmatrix}$. Demonstrate that
 \begin{equation}
 \frac{\partial}{\partial \bm{x}} (\bm{x'Ax}) = (\bm{A} + \bm{A'})\bm{x} \notag
 \end{equation}
and 
 \begin{equation}
 \frac{\partial^2}{\partial \bm{x}\partial \bm{x}'} (\bm{x'Ax}) = \bm{A} + \bm{A'} \notag
 \end{equation}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Multivariate Optimization}
\begin{itemize}
\item First order condition
 \begin{equation}
 \nabla f(\bm{x}) = \bm{0} \notag
 \end{equation}
\vspace{0.2cm}
\item Second order condition... $\rightarrow$ we use the Hessian matrix to determine local min/max!
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Multivariate Optimization (cont.)}
\begin{itemize}
\item Specifically, we examine the sign of the quadratic form of the Hessian matrix evaluated at $\bm{x}^{\star}$ where $\nabla f(\bm{x}^{\star}) = \bm{0}$.
 \begin{itemize}
 \item $\bm{x}'\bm{H}^{\star}\bm{x}$ is the function of $\bm{x}$
 \end{itemize}
\vspace{0.4cm}
\item Second order condition:
 \begin{itemize}
 \item $\bm{x}'\bm{H}^{\star}\bm{x} > 0$ for any values of $\bm{x}$ $\rightarrow$ $\bm{x}^{\star}$ is {\color{red} local min}
  \begin{itemize}
  \item In this case, we say $\bm{H}^{\star}$ is {\color{red} positive definite}
  \end{itemize}
 \item $\bm{x}'\bm{H}^{\star}\bm{x} < 0$ for any values of $\bm{x}$ $\rightarrow$ $\bm{x}^{\star}$ is {\color{red} local max}
  \begin{itemize}
  \item In this case, we say $\bm{H}^{\star}$ is {\color{red} negative definite}
  \end{itemize}
 \item sign of $\bm{x}'\bm{H}^{\star}\bm{x}$ depends on the values of $\bm{x}$ $\rightarrow$ $\bm{x}^{\star}$ is {\color{red} saddle point}
  \begin{itemize}
  \item In this case, we say $\bm{H}^{\star}$ is {\color{red} indefinite}
  \end{itemize}
 \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Multivariate Opimization (cont.)}
\begin{columns}
\begin{column}{0.5\textwidth}
\includegraphics[scale = 0.6]{fig4_1.pdf}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item Saddle point: a point where satisfies the first order condition but neither local minimum nor local maximum
\vspace{0.4cm}
\item Examples
 \begin{itemize}
 \item $f(x) = x^3$
 \item $f(x, y) = x^2 - y^2$
 \end{itemize}
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Multivariate Opimization (cont.)}
\begin{itemize}
\item Example: let $f(x, y) = x^2 - y^2$. Then, as
 \begin{equation}
 \nabla f(x, y) = \begin{pmatrix}
 2x \\
 -2y \\
 \end{pmatrix}, \notag
 \end{equation} 
the point $(x^{\star}, y^{\star})$ satisfies the first order condition. Also, the Hessian matrix is
 \begin{equation}
 \bm{H} = \begin{pmatrix}
 2 & 0 \\
 0 & -2 \\
 \end{pmatrix} \notag
 \end{equation}
\end{itemize}
\end{frame}

\begin{frame}
(\emph{continued from the previous slide})

\vspace{0.4cm}
Therefore, the quadratic form of $\bm{H}$ at $(x^{\star}, y^{\star}) = (0, 0)$ is
 \begin{eqnarray}
 && \begin{pmatrix}
 x & y \\
 \end{pmatrix} \begin{pmatrix}
 2 & 0 \\
 0 & -2 \\
 \end{pmatrix} \begin{pmatrix}
 x \\
 y \\
 \end{pmatrix} \notag \\
 &=& \begin{pmatrix}
 2x & -2y
 \end{pmatrix} \begin{pmatrix}
 x \\
 y \\
 \end{pmatrix} \notag \\
 &=& 2(x^2 - y^2)
 \end{eqnarray}
Since (1) is positive when $|x| > |y|$ and negative $|x| < |y|$, $\bm{H}$ is indefinite, suggesting that $(x^{\star}, y^{\star}) = (0, 0)$ is a saddle point.
\end{frame}

\begin{frame}
\frametitle{Multivariate Opimization: Exercise}
\begin{itemize}
\item Let $f(x, y) = x^2 + xy + y^2$. Demonstrate that $(x, y) = (0, 0)$ is a local minimum of this function. 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Multivariate Optimization (cont.)}
\begin{itemize}
\item Isn't it too cumbersome?
 \begin{itemize}
 \item There's an easier way to determine the definiteness of the Hessian matrix (which uses eigen values)
 \end{itemize}
\vspace{0.4cm}
\item Global v. local
 \begin{itemize}
 \item For $(x^{\star}, y^{\star})$ to be the global min/max, the Hessian matrix must be negative/positive definite at points other than $(x^{\star}, y^{\star})$.
 \item In such cases, $f(\bm{x})$ is globally concave/convex. 
 \end{itemize}
\vspace{0.4cm}
\item (\emph{In most (but not all) of the applications you encounter, you don't need to care about the second order condition...})
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Geometry of Matrix Algebra}
\begin{itemize}
\item All vector/matrix operations have geometric meanings.
\vspace{0.4cm}
\item Here I use examples in two dimensional space, but the discussion naturally extends to $d$-dimensional space. 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Geometry of Matrix Algebra: Vector}
\begin{columns}
\begin{column}{0.5\textwidth}
\centering
\includegraphics[scale = 0.5]{fig4_2.png}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item A $d$-dimensional vector represents a point (more precisely, an arrow to the point) from the origin on the $d$-dimensional coordinate (Eucliean) space.
\vspace{0.4cm}
\item $\bm{u} = \begin{pmatrix}
1 \\
2 \\
\end{pmatrix}, \bm{v} = \begin{pmatrix}
-2 \\
3 \\
\end{pmatrix}$
\vspace{0.4cm}
\item Vector norm represents the length of a vector
 \begin{itemize}
 \item We can show this using the Pythagorean theorem
 \end{itemize}
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Geometry of Matrix Algebra: Vector (cont.)}
\begin{columns}
\begin{column}{0.5\textwidth}
\centering
\only<1>{\includegraphics[scale = 0.5]{fig4_3.png}}
\only<2>{\includegraphics[scale = 0.5]{fig4_4.png}}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item Scalar product: stretching ($|c| > 1$) or contracting ($|c| < 1$) the original vector based on the size of the scalar.
\vspace{0.4cm}
\item When $c < 0$, the original vector is reflected about the origin
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Geometry of Matrix Algebra: Vector (cont.)}
\begin{columns}
\begin{column}{0.5\textwidth}
\centering
\only<1>{\includegraphics[scale = 0.5]{fig4_5.png}}
\only<2>{\includegraphics[scale = 0.5]{fig4_6.png}}
\only<3>{\includegraphics[scale = 0.5]{fig4_7.png}}
\only<4>{\includegraphics[scale = 0.5]{fig4_8.png}}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item Vector addition: move the starting point of the second vector to the end of the first vector, and draw a new arrow from the origin to the end of the second
\vspace{0.4cm}
\item Vector subtraction: multiply the second vector by $-1$ and implement the addition
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Geometry of Matrix Algebra: Vector (cont.)}
\begin{columns}
\begin{column}{0.5\textwidth}
\centering
\only<1>{\includegraphics[scale = 0.5]{fig4_9.png}}
\only<2>{\includegraphics[scale = 0.5]{fig4_10.png}}
\only<3>{\includegraphics[scale = 0.5]{fig4_11.png}}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item Dot/inner product: describes the degree to which one vector overlaps another
\vspace{0.4cm}
\item Product of the length of the first vector and that of the second one projected onto the first
\vspace{0.4cm}
\item Dot product (or length of the projected vector) depends on the angle ($\theta$) between the two
 \begin{itemize}
 \item $\theta = 90^{\circ}$: dot product equals to 0 
 \end{itemize}
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Vector Space}
\begin{itemize}
\item We denote a set of points (i.e., real-valued vectors) on the $d$-dimensional coordinate/Euclidean space $\R^d$.
 \begin{itemize}
 \item $\R$: set of points on a real-number line
 \item $\R^2$: set of points on a 2-D plane
 \item $\R^3$: set of points in 3-D space
 \item ...
 \end{itemize}
\vspace{0.4cm}
\item A set of vectors {\color{red} spans} a (vector) space if every points/vector in that psace can be written as a linear combination of vectors of that set.
 \begin{itemize}
 \item {\color{red} linear combination}: $c_1 \bm{x_1} + c_2 \bm{x_2} + \cdots + c_n \bm{x_n}$
 \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Vector Space (cont.)}
\begin{itemize}
\item When we cannot write any vector in a set as a linear combination of the others, we say they set of vectors is {\color{red} linearly independent}.
\vspace{0.4cm}
\item We call a set of linearly independent vectors which spans a (vector) space a {\color{red} basis}.
 \begin{itemize}
 \item The dimension of a space matches the number of vectors in its basis. 
 \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Vector Space (cont.)}
\begin{columns}
\begin{column}{0.5\textwidth}
\centering
\only<1>{\includegraphics[scale = 0.5]{fig4_12.png}}
\only<2>{\includegraphics[scale = 0.5]{fig4_13.png}}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item Example: $\bm{u} = \begin{pmatrix}
1 \\
0 \\
\end{pmatrix}, \bm{v} = \begin{pmatrix}
0 \\
1 \\
\end{pmatrix}$ spans $\R^2$ because all the points in $\R^2$ can be constructed as a linear combination of them.
 \begin{itemize}
 \item $2\bm{u} - 3\bm{v}$
 \item $-2\bm{u} + 2\bm{v}$
 \item $\bm{u} + 2\bm{v}$
 \item $-\bm{u} - \bm{v}$
 \item ...
 \end{itemize}
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Vector Space (cont.)}
\begin{columns}
\begin{column}{0.5\textwidth}
\centering
\only<1>{\includegraphics[scale = 0.5]{fig4_14.png}}
\only<2>{\includegraphics[scale = 0.5]{fig4_15.png}}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item $\bm{u}$ and $\bm{v}$ form a basis for $\R^2$ as they are linearly independent.
\vspace{0.2cm}
\item They are not the only basis vectors for $\R^2$.
 \begin{itemize}
 \item Example: $\bm{a} = \begin{pmatrix}
 1 \\
 -1 \\
 \end{pmatrix}, \bm{b} = \begin{pmatrix}
 1 \\
 2 \\
 \end{pmatrix}$ spans $\R^2$ and form a basis for $\R^2$
 \begin{itemize}
 \item $\frac{7}{3}\bm{a} - \frac{1}{3}\bm{b}$
 \item $0\bm{a} + \bm{b}$
 \item $-2\bm{a} + 0\bm{b}$
 \item $-\frac{1}{3}\bm{a} - \frac{2}{3}\bm{b}$
 \item ...
 \end{itemize}
 \end{itemize}
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Matrix}
\begin{columns}
\begin{column}{0.5\textwidth}
\centering
\includegraphics[scale = 0.43]{fig4_16.png}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item Geometrically, matrices describe linear transformation of the space/objects on the space
\vspace{0.4cm}
\item Linear transformation: transformation of space while holding the origin 
 \begin{itemize}
 \item rotation
 \item reflection
 \item scaling
 \item squeezing
 \end{itemize}
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Matrix (cont.)}
\begin{columns}
\begin{column}{0.5\textwidth}
\centering
\only<1>{\includegraphics[scale = 0.43]{fig4_16.png}}
\only<2>{\includegraphics[scale = 0.43]{fig4_17.png}}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item Example: Matrix 
 \begin{equation}
 \bm{A} = \begin{pmatrix}
 1 & 3 \\
 2 & 4 \\
 \end{pmatrix} \notag
 \end{equation}
reflects and scales the space up.
 \begin{itemize}
 \item $\bm{Au} = \begin{pmatrix}
 1 \\
 2 \\
 \end{pmatrix}, \bm{Av} = \begin{pmatrix}
 3 \\
 4 \\
 \end{pmatrix}$
 \end{itemize}
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Matrix (cont.)}
\begin{columns}
\begin{column}{0.5\textwidth}
\centering
\only<1>{\includegraphics[scale = 0.43]{fig4_16.png}}
\only<2>{\includegraphics[scale = 0.43]{fig4_18.png}}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item Example: Matrix 
 \begin{equation}
 \bm{B} = \begin{pmatrix}
 3 & -1 \\
 1 & 2 \\
 \end{pmatrix} \notag
 \end{equation}
rotates and scales the space up.
 \begin{itemize}
 \item $\bm{Bu} = \begin{pmatrix}
 3 \\
 1 \\
 \end{pmatrix}, \bm{Bv} = \begin{pmatrix}
 -1 \\
 2 \\
 \end{pmatrix}$
 \end{itemize}
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Determinant}
\begin{columns}
\begin{column}{0.5\textwidth}
\centering
\only<1>{\includegraphics[scale = 0.5]{fig4_19.png}}
\only<2>{\includegraphics[scale = 0.5]{fig4_20.png}}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item Determinant of a square matrix represents the scale factor and the reflection of the linear transformation defined by the matrix.
\vspace{0.4cm}
\item Example: $|\bm{B}| = 7$
 \begin{itemize}
 \item compare the area of rectangles defined by the basis vactors and one of the parallelogram by the transformed vectors
 \end{itemize}
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Determinant (cont.)}
\begin{columns}
\begin{column}{0.5\textwidth}
\centering
\only<1>{\includegraphics[scale = 0.5]{fig4_19.png}}
\only<2>{\includegraphics[scale = 0.5]{fig4_21.png}}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item Example: $|\bm{A}| = -2$
 \begin{itemize}
 \item area of orange parallelogram is 2
 \item negative sign means that the linear transformation defined by $\bm{A}$ reflects the space
 \end{itemize}
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Determinant (cont.)}
\begin{columns}
\begin{column}{0.5\textwidth}
\centering
\only<1>{\includegraphics[scale = 0.5]{fig4_22.png}}
\only<2>{\includegraphics[scale = 0.5]{fig4_23.png}}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item What happens when the determinant equals to 0?
\vspace{0.4cm}
\item Example: The determinant of $\bm{C} = \begin{pmatrix}
 2 & -1 \\
 2 & -1 \\
 \end{pmatrix}$ is 0.
\vspace{0.4cm}
\item Transformed space degenerates to a lower dimensional space!
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Matrix Rank}
\begin{itemize}
\item Rank of a matrix is the number of linearly independent rows/columns.
\vspace{0.4cm}
\item When $\mathrm{rank}(\bm{A}) = \mathrm{min}(n, k)$, we say $\bm{A}$ is {\color{red}{full rank}}.
\vspace{0.4cm}
\item Rank, determinant, invertibility
 \begin{itemize}
 \item when a square matrix $\bm{A}$ is full rank (= all the row/column vectors are linearly independent), $|\bm{A}| \neq 0$, so we can invert the matrix.
 \item when $\bm{A}$ is not full rank, $|\bm{A}| = 0$ and $\bm{A}$ is singular.
 \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matrix Rank (cont.)}
\begin{itemize}
\item Properties of matrix rank
 \begin{enumerate}
 \item $\mathrm{rank}(\bm{A}) = \mathrm{rank}(\bm{A'})$
 \item $\mathrm{rank}(\bm{AA}') = \mathrm{rank}(\bm{A}'\bm{A}) = \mathrm{rank}(\bm{A})$
 \end{enumerate}
\vspace{0.4cm}
\item Practical implications
 \begin{itemize}
 \item if the coefficient matrix $\bm{A}$ of a system of linear equations is singular...
 \item some of its row vectors can be written as a linear combination of others
 \item number  of equations is smaller than the number of unknowns!
 \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Relationship with Statistical Analysis}
\begin{itemize}
\item Dot product as a measure of similarity
 \begin{itemize}
 \item between variables (e.g., correlation coefficient)
 \item between observations (e.g., cosine similarity)
 \end{itemize}
\vspace{0.4cm}
\item Multicollinearity: data matrix is not full rank 
 \begin{itemize}
 \item some column (= variable) can be written as a linear combination of others
 \item $\bm{X}'\bm{X}$ is not full rank either $\rightarrow$ $\bm{X}'\bm{X}$ is singular
 \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Tomorrow}
\begin{itemize}
\item Tomorrow
 \begin{itemize}
 \item Probability
 \item Random variable
 \item Probability distribution
 \item Moore and Siegel, Chapters 9-11.
 \end{itemize}
\end{itemize}
\end{frame}

\end{document}

